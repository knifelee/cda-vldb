{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import time\n",
    "# import collections\n",
    "# import json\n",
    "# from collections import Counter\n",
    "\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import pandasql as pdsql\n",
    "# import miceforest as mf\n",
    "# from warnings import simplefilter\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "# simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "# from utils.util_preprocess import *\n",
    "# #from utils.softimpute import softimpute, cv_softimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = \"/Users/kaiyuli/Documents/Research/DataMarket/_dataset/Student/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fed = pd.read_csv(path_dataset+\"FINREV_FED_17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>idcensus</th>\n",
       "      <th>school_district</th>\n",
       "      <th>nces_id</th>\n",
       "      <th>yr_data</th>\n",
       "      <th>t_fed_rev</th>\n",
       "      <th>c14</th>\n",
       "      <th>c25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>33203100130100</td>\n",
       "      <td>NEW YORK CITY SCHOOL DISTRICT</td>\n",
       "      <td>3620580</td>\n",
       "      <td>17</td>\n",
       "      <td>2061297</td>\n",
       "      <td>956851</td>\n",
       "      <td>439209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5501905900000</td>\n",
       "      <td>LOS ANGELES UNIF SCH DIST</td>\n",
       "      <td>622710</td>\n",
       "      <td>17</td>\n",
       "      <td>1146298</td>\n",
       "      <td>376182</td>\n",
       "      <td>390711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>14501615800000</td>\n",
       "      <td>CITY OF CHICAGO SCHOOL DISTRICT 299</td>\n",
       "      <td>1709930</td>\n",
       "      <td>17</td>\n",
       "      <td>783943</td>\n",
       "      <td>290912</td>\n",
       "      <td>200517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>10501300100000</td>\n",
       "      <td>MIAMI-DADE COUNTY PUBLIC SCHOOL DISTRICT</td>\n",
       "      <td>1200390</td>\n",
       "      <td>17</td>\n",
       "      <td>451479</td>\n",
       "      <td>139461</td>\n",
       "      <td>138098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>39505100100000</td>\n",
       "      <td>PHILADELPHIA SCHOOL DISTRICT</td>\n",
       "      <td>4218990</td>\n",
       "      <td>17</td>\n",
       "      <td>321363</td>\n",
       "      <td>140449</td>\n",
       "      <td>80566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14301</th>\n",
       "      <td>46</td>\n",
       "      <td>46501490400000</td>\n",
       "      <td>BRIDGEWATER POMFRET JOINT CONTRACT SCH DIST</td>\n",
       "      <td>5000389</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14302</th>\n",
       "      <td>47</td>\n",
       "      <td>47211050130100</td>\n",
       "      <td>FAIRFAX CITY SCHOOLS</td>\n",
       "      <td>5101230</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14303</th>\n",
       "      <td>48</td>\n",
       "      <td>48500401200000</td>\n",
       "      <td>STEHEKIN SCHOOL DISTRICT 69</td>\n",
       "      <td>5308430</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14304</th>\n",
       "      <td>48</td>\n",
       "      <td>48501900200000</td>\n",
       "      <td>DAMMAN SCHOOL DISTRICT 7</td>\n",
       "      <td>5301950</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14305</th>\n",
       "      <td>50</td>\n",
       "      <td>50100800830100</td>\n",
       "      <td>CALUMET COUNTY CHILDREN WITH DISABILITIES EDUC...</td>\n",
       "      <td>5500049</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14306 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state_code        idcensus   \n",
       "0              33  33203100130100  \\\n",
       "1               5   5501905900000   \n",
       "2              14  14501615800000   \n",
       "3              10  10501300100000   \n",
       "4              39  39505100100000   \n",
       "...           ...             ...   \n",
       "14301          46  46501490400000   \n",
       "14302          47  47211050130100   \n",
       "14303          48  48500401200000   \n",
       "14304          48  48501900200000   \n",
       "14305          50  50100800830100   \n",
       "\n",
       "                                         school_district  nces_id  yr_data   \n",
       "0                          NEW YORK CITY SCHOOL DISTRICT  3620580       17  \\\n",
       "1                              LOS ANGELES UNIF SCH DIST   622710       17   \n",
       "2                    CITY OF CHICAGO SCHOOL DISTRICT 299  1709930       17   \n",
       "3               MIAMI-DADE COUNTY PUBLIC SCHOOL DISTRICT  1200390       17   \n",
       "4                           PHILADELPHIA SCHOOL DISTRICT  4218990       17   \n",
       "...                                                  ...      ...      ...   \n",
       "14301        BRIDGEWATER POMFRET JOINT CONTRACT SCH DIST  5000389       17   \n",
       "14302                               FAIRFAX CITY SCHOOLS  5101230       17   \n",
       "14303                        STEHEKIN SCHOOL DISTRICT 69  5308430       17   \n",
       "14304                           DAMMAN SCHOOL DISTRICT 7  5301950       17   \n",
       "14305  CALUMET COUNTY CHILDREN WITH DISABILITIES EDUC...  5500049       17   \n",
       "\n",
       "       t_fed_rev     c14     c25  \n",
       "0        2061297  956851  439209  \n",
       "1        1146298  376182  390711  \n",
       "2         783943  290912  200517  \n",
       "3         451479  139461  138098  \n",
       "4         321363  140449   80566  \n",
       "...          ...     ...     ...  \n",
       "14301          0       0       0  \n",
       "14302          0       0       0  \n",
       "14303          0       0       0  \n",
       "14304          0       0       0  \n",
       "14305          0       0       0  \n",
       "\n",
       "[14306 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.921611651758505"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53* 14074 * 5302*2660*2987*26*48 / 10**19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key = pd.read_csv(path_dataset+\"FINREV_FED_KEY_17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State_Code     51\n",
       "State          51\n",
       "#_Records      48\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>State</th>\n",
       "      <th>#_Records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>California</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Florida</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Maine</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Montana</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>New York</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>Utah</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>Washington</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    State_Code                  State  #_Records\n",
       "0             1               Alabama        137\n",
       "1             2                Alaska         54\n",
       "2             3               Arizona        235\n",
       "3             4              Arkansas        250\n",
       "4             5            California       1046\n",
       "5             6              Colorado        197\n",
       "6             7           Connecticut        174\n",
       "7             8              Delaware         19\n",
       "8             9  District of Columbia          1\n",
       "9            10               Florida         67\n",
       "10           11               Georgia        196\n",
       "11           12                Hawaii          1\n",
       "12           13                 Idaho        116\n",
       "13           14              Illinois        983\n",
       "14           15               Indiana        313\n",
       "15           16                  Iowa        342\n",
       "16           17                Kansas        310\n",
       "17           18              Kentucky        173\n",
       "18           19             Louisiana         69\n",
       "19           20                 Maine        253\n",
       "20           21              Maryland         24\n",
       "21           22         Massachusetts        321\n",
       "22           23              Michigan        595\n",
       "23           24             Minnesota        392\n",
       "24           25           Mississippi        144\n",
       "25           26              Missouri        518\n",
       "26           27               Montana        420\n",
       "27           28              Nebraska        262\n",
       "28           29                Nevada         17\n",
       "29           30         New Hampshire        176\n",
       "30           31            New Jersey        585\n",
       "31           32            New Mexico         89\n",
       "32           33              New York        682\n",
       "33           34        North Carolina        116\n",
       "34           35          North Dakota        207\n",
       "35           36                  Ohio        713\n",
       "36           37              Oklahoma        517\n",
       "37           38                Oregon        216\n",
       "38           39          Pennsylvania        596\n",
       "39           40          Rhode Island         39\n",
       "40           41        South Carolina         89\n",
       "41           42          South Dakota        150\n",
       "42           43             Tennessee        142\n",
       "43           44                 Texas       1045\n",
       "44           45                  Utah         41\n",
       "45           46               Vermont        301\n",
       "46           47              Virginia        133\n",
       "47           48            Washington        304\n",
       "48           49         West Virginia         63\n",
       "49           50             Wisconsin        425\n",
       "50           51               Wyoming         48"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nde = pd.read_csv(path_dataset+\"NDECoreExcel_Math_Grade.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>all_students</th>\n",
       "      <th>average_scale_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>National</td>\n",
       "      <td>All students</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>All students</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>All students</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>All students</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>All students</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017</td>\n",
       "      <td>California</td>\n",
       "      <td>All students</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>All students</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>All students</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>All students</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>All students</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>DoDEA</td>\n",
       "      <td>All students</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>Florida</td>\n",
       "      <td>All students</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>All students</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>All students</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>All students</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>All students</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>All students</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>All students</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>All students</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>All students</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>All students</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017</td>\n",
       "      <td>Maine</td>\n",
       "      <td>All students</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>All students</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>All students</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>All students</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>All students</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>All students</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>All students</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017</td>\n",
       "      <td>Montana</td>\n",
       "      <td>All students</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>All students</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>All students</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2017</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>All students</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2017</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>All students</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2017</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>All students</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2017</td>\n",
       "      <td>New York</td>\n",
       "      <td>All students</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2017</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>All students</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2017</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>All students</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2017</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>All students</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2017</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>All students</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2017</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>All students</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2017</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>All students</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2017</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>All students</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2017</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>All students</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2017</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>All students</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2017</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>All students</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2017</td>\n",
       "      <td>Texas</td>\n",
       "      <td>All students</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2017</td>\n",
       "      <td>Utah</td>\n",
       "      <td>All students</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2017</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>All students</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2017</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>All students</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2017</td>\n",
       "      <td>Washington</td>\n",
       "      <td>All students</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2017</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>All students</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2017</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>All students</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2017</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>All students</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year                 state  all_students  average_scale_score\n",
       "0   2017              National  All students                  283\n",
       "1   2017               Alabama  All students                  268\n",
       "2   2017                Alaska  All students                  277\n",
       "3   2017               Arizona  All students                  282\n",
       "4   2017              Arkansas  All students                  274\n",
       "5   2017            California  All students                  277\n",
       "6   2017              Colorado  All students                  286\n",
       "7   2017           Connecticut  All students                  284\n",
       "8   2017              Delaware  All students                  278\n",
       "9   2017  District of Columbia  All students                  266\n",
       "10  2017                 DoDEA  All students                  293\n",
       "11  2017               Florida  All students                  279\n",
       "12  2017               Georgia  All students                  281\n",
       "13  2017                Hawaii  All students                  277\n",
       "14  2017                 Idaho  All students                  284\n",
       "15  2017              Illinois  All students                  282\n",
       "16  2017               Indiana  All students                  288\n",
       "17  2017                  Iowa  All students                  286\n",
       "18  2017                Kansas  All students                  285\n",
       "19  2017              Kentucky  All students                  278\n",
       "20  2017             Louisiana  All students                  267\n",
       "21  2017                 Maine  All students                  284\n",
       "22  2017              Maryland  All students                  281\n",
       "23  2017         Massachusetts  All students                  297\n",
       "24  2017              Michigan  All students                  280\n",
       "25  2017             Minnesota  All students                  294\n",
       "26  2017           Mississippi  All students                  271\n",
       "27  2017              Missouri  All students                  281\n",
       "28  2017               Montana  All students                  286\n",
       "29  2017              Nebraska  All students                  288\n",
       "30  2017                Nevada  All students                  275\n",
       "31  2017         New Hampshire  All students                  293\n",
       "32  2017            New Jersey  All students                  292\n",
       "33  2017            New Mexico  All students                  269\n",
       "34  2017              New York  All students                  282\n",
       "35  2017        North Carolina  All students                  282\n",
       "36  2017          North Dakota  All students                  288\n",
       "37  2017                  Ohio  All students                  288\n",
       "38  2017              Oklahoma  All students                  275\n",
       "39  2017                Oregon  All students                  282\n",
       "40  2017          Pennsylvania  All students                  286\n",
       "41  2017          Rhode Island  All students                  277\n",
       "42  2017        South Carolina  All students                  275\n",
       "43  2017          South Dakota  All students                  286\n",
       "44  2017             Tennessee  All students                  279\n",
       "45  2017                 Texas  All students                  282\n",
       "46  2017                  Utah  All students                  287\n",
       "47  2017               Vermont  All students                  288\n",
       "48  2017              Virginia  All students                  290\n",
       "49  2017            Washington  All students                  289\n",
       "50  2017         West Virginia  All students                  273\n",
       "51  2017             Wisconsin  All students                  288\n",
       "52  2017               Wyoming  All students                  289"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14410"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_fed) + len(df_key) + len(df_nde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess a dataset\n",
    "# check the initialized schema manually first.\n",
    "df = pd.read_csv(path_dataset + \"census13/original.csv\")\n",
    "df['education_num'] = df['education_num'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for k in df.keys():\n",
    "#     print(df[k].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_arr original table to vectors\n",
    "#         integer remains int64\n",
    "#         float remains float64\n",
    "#         others to category\n",
    "# mapping schema\n",
    "df_arr, mapping = df_2_array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rate = 0.3\n",
    "pattern = 0\n",
    "budget = 0.01\n",
    "\n",
    "multiple_imputing_num = 10 # number of models for multiple imputing\n",
    "\n",
    "grid_len_softimputing = 15 # parameter for soft imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [{'col':'age', 'center': 22, 'width':4, 'col_num':0},\n",
    "         {'col':'workclass', 'center': 7, 'width':0, 'col_num':1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate incomplete table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 customed missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_miss incomplete data --- vector\n",
    "# # df_miss                --- dataframe, deep copy of x_miss\n",
    "# mask, x_miss, real_miss_rate = generate_incomplete_table(df_arr.to_numpy(), missing_rate=missing_rate, pattern=pattern)\n",
    "# miss_rows, miss_cols = np.where(np.isnan(x_miss))\n",
    "# df_miss = transform_vector_to_df(x_miss, mapping, df_arr.keys().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 MICE random missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not deep copy df_miss or x_miss\n",
    "# df_miss have the same schema with df_arr\n",
    "#         but all numbers are float64\n",
    "# df_miss and x_miss do not share the same part of memory\n",
    "df_miss = mf.ampute_data(df_arr, perc=missing_rate, random_state=1991)\n",
    "x_miss = df_miss.to_numpy()\n",
    "miss_rows, miss_cols = np.where(np.isnan(x_miss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 random acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df_miss = random_value_acquisition(df_miss, df_arr, budget, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904\n"
     ]
    }
   ],
   "source": [
    "#np.count_nonzero(np.isnan(x_miss))-np.count_nonzero(np.isnan(acquired_x_miss))\n",
    "print(df_miss.isna().sum().sum()-acquired_df_miss.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "formalize_df(acquired_df_miss, mapping, df.keys().to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 greedy acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df_miss = greedy_acquisition(df_miss, df_arr, budget, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901\n"
     ]
    }
   ],
   "source": [
    "#np.count_nonzero(np.isnan(x_miss))-np.count_nonzero(np.isnan(acquired_x_miss))\n",
    "print(df_miss.isna().sum().sum()-acquired_df_miss.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "formalize_df(acquired_df_miss, mapping, df.keys().to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataframe of acquired answer and ground truth\n",
    "data_product = query_on_df(query, acquired_df_miss)\n",
    "ground_truth = query_on_df(query, df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get precision\n",
    "ground_truth_indexes = set(ground_truth.index.values.tolist())\n",
    "data_product_indexes = set(data_product.index.values.tolist())\n",
    "intersect = ground_truth_indexes.intersection(data_product_indexes)\n",
    "precision = len(intersect)/len(ground_truth_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39218442827427674 0.4883720930232558\n"
     ]
    }
   ],
   "source": [
    "# get rmse = precision * RMSE\n",
    "indexes_lst = list(intersect)\n",
    "imputing_error = get_avg_error(df_arr.iloc[indexes_lst], acquired_df_miss.iloc[indexes_lst], mapping)\n",
    "rmse = precision*(1-imputing_error)\n",
    "\n",
    "print(rmse, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "with open(\"queries.json\") as f:\n",
    "    for eachline in f:\n",
    "        query = json.loads(eachline.strip('\\n'))\n",
    "        queries.append(query['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 269.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 282.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 284.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 266.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 282.77it/s]\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "precisions = []\n",
    "rmses = []\n",
    "idx = []\n",
    "for budget in range(5):\n",
    "    budget = (budget+1)*0.01\n",
    "\n",
    "    # acquisition results : acquired_df_miss\n",
    "    st = time.time()\n",
    "    acquired_df_miss = random_value_acquisition(df_miss, df_arr, budget, query)\n",
    "    et = time.time()\n",
    "\n",
    "    formalize_df(acquired_df_miss, mapping, df.keys().to_list())\n",
    "    \n",
    "    pres = 0\n",
    "    rs = 0\n",
    "    \n",
    "    for query in tqdm(queries):\n",
    "        # get the dataframe of acquired answer and ground truth\n",
    "        data_product = query_on_df(query, acquired_df_miss)\n",
    "        ground_truth = query_on_df(query, df_arr)\n",
    "\n",
    "        # get precision\n",
    "        ground_truth_indexes = set(ground_truth.index.values.tolist())\n",
    "        data_product_indexes = set(data_product.index.values.tolist())\n",
    "        intersect = ground_truth_indexes.intersection(data_product_indexes)\n",
    "        precision = len(intersect)/len(ground_truth_indexes)\n",
    "\n",
    "        # get rmse = precision * RMSE\n",
    "        indexes_lst = list(intersect)\n",
    "        imputing_error = get_avg_error(df_arr.iloc[indexes_lst], acquired_df_miss.iloc[indexes_lst], mapping)\n",
    "        rmse = precision*(1-imputing_error)\n",
    "        \n",
    "        pres += precision\n",
    "        rs += rmse\n",
    "    \n",
    "    idx.append(budget)\n",
    "    times.append(et-st)\n",
    "    precisions.append(pres/len(queries))\n",
    "    rmses.append(rs/len(queries))\n",
    "\n",
    "result = pd.DataFrame({\"time\": times, \"precision\": precisions, \"rmse\":rmses}, index=idx)\n",
    "result.to_csv(\"greedy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038386789169023647"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████████▎                                                                                                                                                                                               | 1/10 [23:19<3:29:58, 1399.86s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████████████▌                                                                                                                                                                          | 2/10 [45:00<2:58:50, 1341.30s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████████████████████████████████▎                                                                                                                                                   | 3/10 [1:08:40<2:40:42, 1377.49s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                              | 4/10 [1:31:43<2:17:57, 1379.53s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                         | 5/10 [1:54:37<1:54:48, 1377.63s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                    | 6/10 [2:18:28<1:33:03, 1395.77s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                               | 7/10 [2:40:01<1:08:06, 1362.15s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 8/10 [3:03:57<46:11, 1385.56s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 9/10 [3:27:51<23:20, 1400.67s/it]/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-3\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 2\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 3\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 4\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "2  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "3  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [3:51:39<00:00, 1389.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# with open(\"UNBA.csv\", 'w') as f:\n",
    "times = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "F1s = []\n",
    "rmses = []\n",
    "for budget in [0.01]:\n",
    "    ts = 0\n",
    "    pres = 0\n",
    "    recs = 0\n",
    "    fs = 0\n",
    "    rs = 0\n",
    "    \n",
    "    # acquisition results : acquired_df_miss\n",
    "    st = time.time()\n",
    "    acquired_df_miss = one_pass_max_uncertainty(df_miss, df_arr, budget, query)\n",
    "    et = time.time()\n",
    "    \n",
    "    for query in tqdm(queries):\n",
    "        \n",
    "        imputed_data = IterativeImputer(random_state=0, max_iter=50).fit_transform(acquired_df_miss)\n",
    "        df_imputed = pd.DataFrame(imputed_data, columns = df.keys().to_list())\n",
    "        formalize_imputed_df(df_imputed, mapping, df.keys().to_list())\n",
    "\n",
    "        data_product = query_on_df(query, df_imputed)\n",
    "        ground_truth = query_on_df(query, df_arr)\n",
    "\n",
    "        ground_truth_indexes = set(ground_truth.index.values.tolist())\n",
    "        data_product_indexes = set(data_product.index.values.tolist())\n",
    "        intersect = ground_truth_indexes.intersection(data_product_indexes)\n",
    "        indexes_lst = list(intersect)\n",
    "        imputing_error = get_avg_error(df_arr.iloc[indexes_lst], df_imputed.iloc[indexes_lst], mapping)\n",
    "        precision = len(intersect)/len(ground_truth_indexes)\n",
    "        recall = len(intersect)/len(data_product_indexes)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        rmse = F1*(1-imputing_error)\n",
    "\n",
    "        ts += (et-st)\n",
    "        pres += precision\n",
    "        recs += recall\n",
    "        fs += F1\n",
    "        rs += rmse\n",
    "\n",
    "    times.append(ts/10)\n",
    "    precisions.append(pres/10)\n",
    "    recalls.append(recs/10)\n",
    "    F1s.append(fs/10)\n",
    "    rmses.append(rs/10)\n",
    "#     f.write(str(times[-1])+'\\t'+\n",
    "#             str(precisions[-1])+'\\t'+\n",
    "#             str(recalls[-1])+'\\t'+\n",
    "#             str(F1s[-1])+'\\t'+\n",
    "#             str(rmses[-1])+'\\t'+\n",
    "#             '\\n')\n",
    "\n",
    "# ts += (et-st)\n",
    "# pres += pres\n",
    "# rs += rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = pd.DataFrame(data={'budget':[0.01], 'time':times, 'precision':precisions, 'recall':recalls, 'F1':F1s, 'rmse':rmses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random.to_csv(\"UNBA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 expectation maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "Dataset 1\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n"
     ]
    }
   ],
   "source": [
    "acquired_df_miss = one_pass_max_uncertainty(df_miss, df_arr, budget, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'col': 'age', 'center': 22, 'width': 4, 'col_num': 0},\n",
       " {'col': 'workclass', 'center': 7, 'width': 0, 'col_num': 1}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df_miss = random_sample_acquisition(df_miss, df_arr, budget, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss_copy = df_miss.copy()\n",
    "budget_units = int(budget*len(miss_rows))+1\n",
    "query_columns = [q['col_num'] for q in query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | age | workclass | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "102 1788\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "116 1653\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "90 1543\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "91 1434\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "67 1350\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "83 1250\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "67 1168\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "55 1093\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "46 1035\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "47 974\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "50 914\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "46 853\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "46 794\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "31 747\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "40 694\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "41 637\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "34 589\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "35 541\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "43 484\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "34 436\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "33 388\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "41 335\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "34 283\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "37 229\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "41 165\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "33 115\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "30 69\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "21 36\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "20 8\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "23 -27\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "26 -65\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "31 -115\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "23 -147\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "33 -198\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "30 -244\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "29 -293\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "27 -337\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "13 -355\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "17 -383\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "16 -408\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "24 -447\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "21 -480\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "21 -512\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "10 -529\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "27 -575\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "21 -611\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "23 -649\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "30 -694\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | age | education | education_num | marital_status | occupation | relationship | race | sex | capital_gain | capital_loss | hours_per_week | native_country\n",
      "21 -728\n",
      "Initialized logger with name mice 1-1\n",
      "Dataset 0\n",
      "1  | workclass"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiyuli/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:371: UserWarning: [workclass,education,education_num,marital_status,occupation,native_country] have very rare categories, it is a good idea to group these, or set the min_data_in_leaf parameter to prevent lightgbm from outputting 0.0 probabilities.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m kernel \u001b[38;5;241m=\u001b[39m mf\u001b[38;5;241m.\u001b[39mImputationKernel(\n\u001b[1;32m      8\u001b[0m   data\u001b[38;5;241m=\u001b[39mdf_miss_copy,\n\u001b[1;32m      9\u001b[0m   datasets\u001b[38;5;241m=\u001b[39mmulti_imp_num,\n\u001b[1;32m     10\u001b[0m   save_all_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m   random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1991\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Run the MICE algorithm for 3 iterations on each of the datasets\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_imp_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_data_in_leaf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m acq_task \u001b[38;5;241m=\u001b[39m get_utility_score(kernel, query, candidate_rows, \u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(acq_task)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/miceforest/ImputationKernel.py:1114\u001b[0m, in \u001b[0;36mImputationKernel.mice\u001b[0;34m(self, iterations, verbose, variable_parameters, compile_candidates, **kwlgb)\u001b[0m\n\u001b[1;32m   1112\u001b[0m logger\u001b[38;5;241m.\u001b[39mrecord_time(timed_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_xy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlog_context)\n\u001b[1;32m   1113\u001b[0m logger\u001b[38;5;241m.\u001b[39mset_start_time()\n\u001b[0;32m-> 1114\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlgbpars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_pointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_cat_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m logger\u001b[38;5;241m.\u001b[39mrecord_time(timed_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlog_context)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_model:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while budget_units>0:\n",
    "    \n",
    "    _rows, _cols = np.where(np.isnan(df_miss_copy.to_numpy()))\n",
    "    candidate_rows = get_candidate_rows(query_columns, _rows, _cols)\n",
    "\n",
    "    # Create kernels. \n",
    "    kernel = mf.ImputationKernel(\n",
    "      data=df_miss_copy,\n",
    "      datasets=multi_imp_num,\n",
    "      save_all_iterations=False,\n",
    "      random_state=1991,\n",
    "    )\n",
    "\n",
    "    # Run the MICE algorithm for 3 iterations on each of the datasets\n",
    "    kernel.mice(multi_imp_iter, verbose=True, min_data_in_leaf = 6)\n",
    "    acq_task = get_utility_score(kernel, query, candidate_rows, 500)\n",
    "    \n",
    "    if len(acq_task)!=0:\n",
    "        for line in acq_task:\n",
    "            for c in query_columns:\n",
    "                if pd.isna(df_miss_copy.iat[line[0], c])==True:\n",
    "                    budget_units -= 1\n",
    "                    df_miss_copy.iat[line[0],c] = df_arr.iat[line[0],c]\n",
    "    else:\n",
    "        acq_task = random.sample(candidate_rows, budget)\n",
    "        for line in acq_task:\n",
    "            for c in query_columns:\n",
    "                if pd.isna(df_miss_copy.iat[line, c])==True:\n",
    "                    budget_units -= 1\n",
    "                    df_miss_copy.iat[line,c] = df_arr.iat[line,c]\n",
    "                    \n",
    "    print(len(acq_task), budget_units) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(41512, 16),\n",
       " (34415, 12),\n",
       " (35934, 12),\n",
       " (38365, 12),\n",
       " (22105, 11),\n",
       " (22163, 11),\n",
       " (22616, 11),\n",
       " (23887, 11),\n",
       " (39629, 11),\n",
       " (43742, 11),\n",
       " (2225, 10),\n",
       " (19839, 10),\n",
       " (30829, 10),\n",
       " (36756, 10),\n",
       " (46052, 10),\n",
       " (47986, 10),\n",
       " (2280, 9),\n",
       " (25593, 9),\n",
       " (29584, 9),\n",
       " (39462, 9),\n",
       " (42883, 9),\n",
       " (46607, 9),\n",
       " (280, 8),\n",
       " (1434, 8),\n",
       " (10092, 8),\n",
       " (12211, 8),\n",
       " (16680, 8),\n",
       " (19033, 8),\n",
       " (21965, 8),\n",
       " (23385, 8),\n",
       " (27255, 8),\n",
       " (29126, 8),\n",
       " (31174, 8),\n",
       " (32248, 8),\n",
       " (36212, 8),\n",
       " (39512, 8),\n",
       " (45355, 8),\n",
       " (2636, 7),\n",
       " (4163, 7),\n",
       " (5055, 7),\n",
       " (5515, 7),\n",
       " (5715, 7),\n",
       " (8514, 7),\n",
       " (9702, 7),\n",
       " (13134, 7),\n",
       " (14029, 7),\n",
       " (15613, 7),\n",
       " (18605, 7),\n",
       " (22912, 7),\n",
       " (23923, 7),\n",
       " (24500, 7),\n",
       " (24865, 7),\n",
       " (29337, 7),\n",
       " (29903, 7),\n",
       " (30003, 7),\n",
       " (31143, 7),\n",
       " (32191, 7),\n",
       " (32605, 7),\n",
       " (38817, 7),\n",
       " (620, 6),\n",
       " (1602, 6),\n",
       " (3566, 6),\n",
       " (4598, 6),\n",
       " (7486, 6),\n",
       " (8388, 6),\n",
       " (10336, 6),\n",
       " (12695, 6),\n",
       " (15367, 6),\n",
       " (23062, 6),\n",
       " (24585, 6),\n",
       " (28299, 6),\n",
       " (30392, 6),\n",
       " (33770, 6),\n",
       " (37955, 6),\n",
       " (43352, 6),\n",
       " (43894, 6),\n",
       " (45058, 6),\n",
       " (46268, 6),\n",
       " (48061, 6),\n",
       " (48469, 6),\n",
       " (236, 5),\n",
       " (8349, 5),\n",
       " (8623, 5),\n",
       " (9337, 5),\n",
       " (17879, 5),\n",
       " (20564, 5),\n",
       " (26391, 5),\n",
       " (27148, 5),\n",
       " (27729, 5),\n",
       " (29233, 5),\n",
       " (30323, 5),\n",
       " (35898, 5),\n",
       " (37278, 5),\n",
       " (37443, 5),\n",
       " (37710, 5),\n",
       " (37763, 5),\n",
       " (40234, 5),\n",
       " (40528, 5),\n",
       " (41829, 5),\n",
       " (42632, 5),\n",
       " (45483, 5),\n",
       " (47155, 5),\n",
       " (4939, 4),\n",
       " (6259, 4),\n",
       " (7853, 4),\n",
       " (7953, 4),\n",
       " (10943, 4),\n",
       " (11935, 4),\n",
       " (12233, 4),\n",
       " (12788, 4),\n",
       " (16085, 4),\n",
       " (18749, 4),\n",
       " (19701, 4),\n",
       " (21277, 4),\n",
       " (21905, 4),\n",
       " (23148, 4),\n",
       " (24212, 4),\n",
       " (27570, 4),\n",
       " (27634, 4),\n",
       " (29151, 4),\n",
       " (29252, 4),\n",
       " (30554, 4),\n",
       " (30817, 4),\n",
       " (31387, 4),\n",
       " (31436, 4),\n",
       " (31753, 4),\n",
       " (32126, 4),\n",
       " (34197, 4),\n",
       " (35040, 4),\n",
       " (35655, 4),\n",
       " (37657, 4),\n",
       " (38027, 4),\n",
       " (38265, 4),\n",
       " (39118, 4),\n",
       " (39269, 4),\n",
       " (39856, 4),\n",
       " (41435, 4),\n",
       " (41631, 4),\n",
       " (45246, 4),\n",
       " (47194, 4),\n",
       " (47395, 4),\n",
       " (47827, 4),\n",
       " (419, 3),\n",
       " (2221, 3),\n",
       " (3151, 3),\n",
       " (3440, 3),\n",
       " (3896, 3),\n",
       " (4919, 3),\n",
       " (5131, 3),\n",
       " (5506, 3),\n",
       " (5962, 3),\n",
       " (6315, 3),\n",
       " (7007, 3),\n",
       " (7214, 3),\n",
       " (7744, 3),\n",
       " (7793, 3),\n",
       " (9258, 3),\n",
       " (9499, 3),\n",
       " (9952, 3),\n",
       " (11488, 3),\n",
       " (12052, 3),\n",
       " (12097, 3),\n",
       " (12106, 3),\n",
       " (13411, 3),\n",
       " (13579, 3),\n",
       " (13718, 3),\n",
       " (14037, 3),\n",
       " (14488, 3),\n",
       " (14635, 3),\n",
       " (15213, 3),\n",
       " (15264, 3),\n",
       " (15536, 3),\n",
       " (16084, 3),\n",
       " (16262, 3),\n",
       " (17341, 3),\n",
       " (17951, 3),\n",
       " (18244, 3),\n",
       " (19732, 3),\n",
       " (20476, 3),\n",
       " (21581, 3),\n",
       " (23357, 3),\n",
       " (23440, 3),\n",
       " (23985, 3),\n",
       " (24042, 3),\n",
       " (24066, 3),\n",
       " (25587, 3),\n",
       " (25609, 3),\n",
       " (25925, 3),\n",
       " (27004, 3),\n",
       " (27134, 3),\n",
       " (27962, 3),\n",
       " (28804, 3),\n",
       " (29309, 3),\n",
       " (30000, 3),\n",
       " (30150, 3),\n",
       " (30327, 3),\n",
       " (30461, 3),\n",
       " (30650, 3),\n",
       " (30849, 3),\n",
       " (31574, 3),\n",
       " (31837, 3),\n",
       " (31934, 3),\n",
       " (32479, 3),\n",
       " (35442, 3),\n",
       " (35497, 3),\n",
       " (35558, 3),\n",
       " (36426, 3),\n",
       " (36688, 3),\n",
       " (37227, 3),\n",
       " (37622, 3),\n",
       " (37714, 3),\n",
       " (38324, 3),\n",
       " (38667, 3),\n",
       " (39185, 3),\n",
       " (39453, 3),\n",
       " (39531, 3),\n",
       " (40361, 3),\n",
       " (41139, 3),\n",
       " (41372, 3),\n",
       " (41508, 3),\n",
       " (41912, 3),\n",
       " (42158, 3),\n",
       " (42313, 3),\n",
       " (42735, 3),\n",
       " (42988, 3),\n",
       " (44596, 3),\n",
       " (47236, 3),\n",
       " (47335, 3),\n",
       " (47439, 3),\n",
       " (47925, 3),\n",
       " (48707, 3),\n",
       " (36, 2),\n",
       " (48, 2),\n",
       " (120, 2),\n",
       " (700, 2),\n",
       " (1003, 2),\n",
       " (1572, 2),\n",
       " (1866, 2),\n",
       " (2423, 2),\n",
       " (3097, 2),\n",
       " (3357, 2),\n",
       " (3996, 2),\n",
       " (4577, 2),\n",
       " (4619, 2),\n",
       " (4992, 2),\n",
       " (5381, 2),\n",
       " (5618, 2),\n",
       " (5770, 2),\n",
       " (6044, 2),\n",
       " (6053, 2),\n",
       " (6262, 2),\n",
       " (6400, 2),\n",
       " (6664, 2),\n",
       " (7019, 2),\n",
       " (7037, 2),\n",
       " (7304, 2),\n",
       " (7316, 2),\n",
       " (7855, 2),\n",
       " (8281, 2),\n",
       " (8445, 2),\n",
       " (8508, 2),\n",
       " (8527, 2),\n",
       " (8837, 2),\n",
       " (9010, 2),\n",
       " (9077, 2),\n",
       " (9203, 2),\n",
       " (9227, 2),\n",
       " (9368, 2),\n",
       " (9411, 2),\n",
       " (9552, 2),\n",
       " (9713, 2),\n",
       " (9929, 2),\n",
       " (9950, 2),\n",
       " (10526, 2),\n",
       " (10918, 2),\n",
       " (11471, 2),\n",
       " (11661, 2),\n",
       " (11813, 2),\n",
       " (11916, 2),\n",
       " (12069, 2),\n",
       " (12084, 2),\n",
       " (12240, 2),\n",
       " (12534, 2),\n",
       " (12772, 2),\n",
       " (12774, 2),\n",
       " (12847, 2),\n",
       " (13990, 2),\n",
       " (14069, 2),\n",
       " (14453, 2),\n",
       " (14583, 2),\n",
       " (15097, 2),\n",
       " (15639, 2),\n",
       " (15867, 2),\n",
       " (15900, 2),\n",
       " (16125, 2),\n",
       " (16818, 2),\n",
       " (17226, 2),\n",
       " (17386, 2),\n",
       " (17696, 2),\n",
       " (18526, 2),\n",
       " (18614, 2),\n",
       " (18724, 2),\n",
       " (19209, 2),\n",
       " (19331, 2),\n",
       " (20451, 2),\n",
       " (20707, 2),\n",
       " (20721, 2),\n",
       " (20811, 2),\n",
       " (21031, 2),\n",
       " (21178, 2),\n",
       " (21278, 2),\n",
       " (21738, 2),\n",
       " (21795, 2),\n",
       " (21832, 2),\n",
       " (22092, 2),\n",
       " (22344, 2),\n",
       " (22369, 2),\n",
       " (22371, 2),\n",
       " (22509, 2),\n",
       " (23193, 2),\n",
       " (23194, 2),\n",
       " (23408, 2),\n",
       " (23596, 2),\n",
       " (23787, 2),\n",
       " (23981, 2),\n",
       " (24011, 2),\n",
       " (24671, 2),\n",
       " (24962, 2),\n",
       " (25583, 2),\n",
       " (25760, 2),\n",
       " (25907, 2),\n",
       " (25913, 2),\n",
       " (26137, 2),\n",
       " (26558, 2),\n",
       " (26916, 2),\n",
       " (26997, 2),\n",
       " (27048, 2),\n",
       " (27191, 2),\n",
       " (27209, 2),\n",
       " (27408, 2),\n",
       " (28115, 2),\n",
       " (28133, 2),\n",
       " (29474, 2),\n",
       " (29534, 2),\n",
       " (29739, 2),\n",
       " (29943, 2),\n",
       " (30064, 2),\n",
       " (30236, 2),\n",
       " (30443, 2),\n",
       " (30471, 2),\n",
       " (31008, 2),\n",
       " (31089, 2),\n",
       " (31176, 2),\n",
       " (31578, 2),\n",
       " (31856, 2),\n",
       " (32118, 2),\n",
       " (32120, 2),\n",
       " (33074, 2),\n",
       " (33250, 2),\n",
       " (33863, 2),\n",
       " (34150, 2),\n",
       " (34770, 2),\n",
       " (34822, 2),\n",
       " (35257, 2),\n",
       " (35398, 2),\n",
       " (35801, 2),\n",
       " (36398, 2),\n",
       " (36761, 2),\n",
       " (36926, 2),\n",
       " (37317, 2),\n",
       " (37882, 2),\n",
       " (37966, 2),\n",
       " (38072, 2),\n",
       " (38648, 2),\n",
       " (39082, 2),\n",
       " (39290, 2),\n",
       " (39315, 2),\n",
       " (39636, 2),\n",
       " (39792, 2),\n",
       " (39887, 2),\n",
       " (40107, 2),\n",
       " (40114, 2),\n",
       " (40564, 2),\n",
       " (40645, 2),\n",
       " (40868, 2),\n",
       " (40998, 2),\n",
       " (41094, 2),\n",
       " (41373, 2),\n",
       " (41674, 2),\n",
       " (41926, 2),\n",
       " (42604, 2),\n",
       " (42842, 2),\n",
       " (42981, 2),\n",
       " (43305, 2),\n",
       " (43388, 2),\n",
       " (43449, 2),\n",
       " (44172, 2),\n",
       " (45148, 2),\n",
       " (45851, 2),\n",
       " (46149, 2),\n",
       " (46197, 2),\n",
       " (47043, 2),\n",
       " (47512, 2),\n",
       " (47919, 2),\n",
       " (48026, 2),\n",
       " (260, 1),\n",
       " (420, 1),\n",
       " (683, 1),\n",
       " (730, 1),\n",
       " (866, 1),\n",
       " (1179, 1),\n",
       " (1244, 1),\n",
       " (1305, 1),\n",
       " (1472, 1),\n",
       " (1693, 1),\n",
       " (1947, 1),\n",
       " (2011, 1),\n",
       " (2049, 1),\n",
       " (2091, 1),\n",
       " (2253, 1),\n",
       " (2471, 1),\n",
       " (2499, 1),\n",
       " (2609, 1),\n",
       " (2699, 1),\n",
       " (2788, 1),\n",
       " (2873, 1),\n",
       " (2952, 1),\n",
       " (3057, 1),\n",
       " (3109, 1),\n",
       " (3112, 1),\n",
       " (3417, 1),\n",
       " (3468, 1),\n",
       " (3507, 1),\n",
       " (3632, 1),\n",
       " (3686, 1),\n",
       " (3764, 1),\n",
       " (3770, 1),\n",
       " (3862, 1),\n",
       " (3951, 1),\n",
       " (3973, 1),\n",
       " (4056, 1),\n",
       " (4089, 1),\n",
       " (4150, 1),\n",
       " (4457, 1),\n",
       " (4615, 1),\n",
       " (4679, 1),\n",
       " (4690, 1),\n",
       " (4892, 1),\n",
       " (4957, 1),\n",
       " (5080, 1),\n",
       " (5119, 1),\n",
       " (5185, 1),\n",
       " (5215, 1),\n",
       " (5230, 1),\n",
       " (5339, 1),\n",
       " (5348, 1),\n",
       " (5375, 1),\n",
       " (5386, 1),\n",
       " (5412, 1),\n",
       " (5415, 1),\n",
       " (5519, 1),\n",
       " (5594, 1),\n",
       " (5697, 1),\n",
       " (5914, 1),\n",
       " (5945, 1),\n",
       " (6043, 1),\n",
       " (6083, 1),\n",
       " (6166, 1),\n",
       " (6228, 1),\n",
       " (6241, 1),\n",
       " (6286, 1),\n",
       " (6443, 1),\n",
       " (6637, 1),\n",
       " (6696, 1),\n",
       " (6771, 1),\n",
       " (6856, 1),\n",
       " (6929, 1),\n",
       " (6940, 1),\n",
       " (7067, 1),\n",
       " (7165, 1),\n",
       " (7180, 1),\n",
       " (7458, 1),\n",
       " (7512, 1),\n",
       " (7525, 1),\n",
       " (7539, 1),\n",
       " (7551, 1),\n",
       " (7612, 1),\n",
       " (7857, 1),\n",
       " (7877, 1),\n",
       " (7903, 1),\n",
       " (8144, 1),\n",
       " (8186, 1),\n",
       " (8190, 1),\n",
       " (8241, 1),\n",
       " (8313, 1),\n",
       " (8465, 1),\n",
       " (8708, 1),\n",
       " (8744, 1),\n",
       " (8820, 1),\n",
       " (8831, 1),\n",
       " (8876, 1),\n",
       " (8904, 1),\n",
       " (8986, 1),\n",
       " (9013, 1),\n",
       " (9078, 1),\n",
       " (9082, 1),\n",
       " (9094, 1),\n",
       " (9239, 1),\n",
       " (9257, 1),\n",
       " (9263, 1),\n",
       " (9343, 1),\n",
       " (9405, 1),\n",
       " (9470, 1),\n",
       " (9496, 1),\n",
       " (9527, 1),\n",
       " (9563, 1),\n",
       " (9577, 1),\n",
       " (9597, 1),\n",
       " (9683, 1),\n",
       " (9793, 1),\n",
       " (9860, 1),\n",
       " (9912, 1),\n",
       " (10037, 1),\n",
       " (10224, 1),\n",
       " (10364, 1),\n",
       " (10394, 1),\n",
       " (10472, 1),\n",
       " (10537, 1),\n",
       " (10631, 1),\n",
       " (10846, 1),\n",
       " (10863, 1),\n",
       " (10895, 1),\n",
       " (10970, 1),\n",
       " (10995, 1),\n",
       " (11041, 1),\n",
       " (11108, 1),\n",
       " (11167, 1),\n",
       " (11221, 1),\n",
       " (11270, 1),\n",
       " (11277, 1),\n",
       " (11317, 1),\n",
       " (11481, 1),\n",
       " (11549, 1),\n",
       " (11743, 1),\n",
       " (11826, 1),\n",
       " (12174, 1),\n",
       " (12329, 1),\n",
       " (12388, 1),\n",
       " (12430, 1),\n",
       " (12701, 1),\n",
       " (12765, 1),\n",
       " (12815, 1),\n",
       " (12848, 1),\n",
       " (13003, 1),\n",
       " (13014, 1),\n",
       " (13231, 1),\n",
       " (13324, 1),\n",
       " (13339, 1),\n",
       " (13902, 1),\n",
       " (14023, 1),\n",
       " (14497, 1),\n",
       " (14557, 1),\n",
       " (14608, 1),\n",
       " (14640, 1),\n",
       " (14648, 1),\n",
       " (14805, 1),\n",
       " (14939, 1),\n",
       " (14945, 1),\n",
       " (14964, 1),\n",
       " (14965, 1),\n",
       " (14994, 1),\n",
       " (15073, 1),\n",
       " (15184, 1),\n",
       " (15201, 1),\n",
       " (15237, 1),\n",
       " (15316, 1),\n",
       " (15445, 1),\n",
       " (15451, 1),\n",
       " (15510, 1),\n",
       " (15562, 1),\n",
       " (15663, 1),\n",
       " (15851, 1),\n",
       " (15855, 1),\n",
       " (15872, 1),\n",
       " (15958, 1),\n",
       " (16054, 1),\n",
       " (16135, 1),\n",
       " (16347, 1),\n",
       " (16357, 1),\n",
       " (16408, 1),\n",
       " (16430, 1),\n",
       " (16493, 1),\n",
       " (16560, 1),\n",
       " (16587, 1),\n",
       " (16698, 1),\n",
       " (16746, 1),\n",
       " (16777, 1),\n",
       " (16955, 1),\n",
       " (16993, 1),\n",
       " (17009, 1),\n",
       " (17045, 1),\n",
       " (17131, 1),\n",
       " (17359, 1),\n",
       " (17468, 1),\n",
       " (17536, 1),\n",
       " (17552, 1),\n",
       " (17730, 1),\n",
       " (17779, 1),\n",
       " (17863, 1),\n",
       " (17930, 1),\n",
       " (17946, 1),\n",
       " (17993, 1),\n",
       " (17994, 1),\n",
       " (18220, 1),\n",
       " (18246, 1),\n",
       " (18316, 1),\n",
       " (18374, 1),\n",
       " (18421, 1),\n",
       " (18472, 1),\n",
       " (18474, 1),\n",
       " (18517, 1),\n",
       " (18587, 1),\n",
       " (18655, 1),\n",
       " (18821, 1),\n",
       " (18889, 1),\n",
       " (18919, 1),\n",
       " (19010, 1),\n",
       " (19048, 1),\n",
       " (19092, 1),\n",
       " (19149, 1),\n",
       " (19374, 1),\n",
       " (19411, 1),\n",
       " (19427, 1),\n",
       " (19522, 1),\n",
       " (19545, 1),\n",
       " (19547, 1),\n",
       " (19560, 1),\n",
       " (19569, 1),\n",
       " (19667, 1),\n",
       " (19724, 1),\n",
       " (19782, 1),\n",
       " (19824, 1),\n",
       " (19884, 1),\n",
       " (19939, 1),\n",
       " (20066, 1),\n",
       " (20341, 1),\n",
       " (20380, 1),\n",
       " (20406, 1),\n",
       " (20530, 1),\n",
       " (20626, 1),\n",
       " (20663, 1),\n",
       " (20727, 1),\n",
       " (20812, 1),\n",
       " (20883, 1),\n",
       " (20901, 1),\n",
       " (20922, 1),\n",
       " (20938, 1),\n",
       " (20971, 1),\n",
       " (21041, 1),\n",
       " (21869, 1),\n",
       " (22014, 1),\n",
       " (22119, 1),\n",
       " (22172, 1),\n",
       " (22300, 1),\n",
       " (22318, 1),\n",
       " (22398, 1),\n",
       " (22564, 1),\n",
       " (22605, 1),\n",
       " (22779, 1),\n",
       " (22981, 1),\n",
       " (23027, 1),\n",
       " (23067, 1),\n",
       " (23096, 1),\n",
       " (23144, 1),\n",
       " (23162, 1),\n",
       " (23453, 1),\n",
       " (23492, 1),\n",
       " (23605, 1),\n",
       " (23741, 1),\n",
       " (23993, 1),\n",
       " (24216, 1),\n",
       " (24377, 1),\n",
       " (24623, 1),\n",
       " (24625, 1),\n",
       " (24781, 1),\n",
       " (25097, 1),\n",
       " (25157, 1),\n",
       " (25165, 1),\n",
       " (25179, 1),\n",
       " (25474, 1),\n",
       " (25565, 1),\n",
       " (25763, 1),\n",
       " (25934, 1),\n",
       " (25978, 1),\n",
       " (25987, 1),\n",
       " (26062, 1),\n",
       " (26124, 1),\n",
       " (26158, 1),\n",
       " (26161, 1),\n",
       " (26171, 1),\n",
       " (26205, 1),\n",
       " (26214, 1),\n",
       " (26417, 1),\n",
       " (26520, 1),\n",
       " (26573, 1),\n",
       " (26581, 1),\n",
       " (26603, 1),\n",
       " (26744, 1),\n",
       " (26882, 1),\n",
       " (27269, 1),\n",
       " (27294, 1),\n",
       " (27496, 1),\n",
       " (27632, 1),\n",
       " (27650, 1),\n",
       " (27673, 1),\n",
       " (28163, 1),\n",
       " (28276, 1),\n",
       " (28398, 1),\n",
       " (28597, 1),\n",
       " (28722, 1),\n",
       " (28753, 1),\n",
       " (28773, 1),\n",
       " (28780, 1),\n",
       " (29116, 1),\n",
       " (29205, 1),\n",
       " (29218, 1),\n",
       " (29326, 1),\n",
       " (29373, 1),\n",
       " (29512, 1),\n",
       " (29731, 1),\n",
       " (29770, 1),\n",
       " (29919, 1),\n",
       " (30025, 1),\n",
       " (30028, 1),\n",
       " (30032, 1),\n",
       " (30225, 1),\n",
       " (30271, 1),\n",
       " (30275, 1),\n",
       " (30313, 1),\n",
       " (30328, 1),\n",
       " (30341, 1),\n",
       " (30355, 1),\n",
       " (30405, 1),\n",
       " (30457, 1),\n",
       " (30531, 1),\n",
       " (30575, 1),\n",
       " (30583, 1),\n",
       " (30601, 1),\n",
       " (30660, 1),\n",
       " (30733, 1),\n",
       " (30770, 1),\n",
       " (30777, 1),\n",
       " (30793, 1),\n",
       " (30836, 1),\n",
       " (30866, 1),\n",
       " (30972, 1),\n",
       " (31036, 1),\n",
       " (31052, 1),\n",
       " (31154, 1),\n",
       " (31193, 1),\n",
       " (31251, 1),\n",
       " (31458, 1),\n",
       " (31588, 1),\n",
       " (31687, 1),\n",
       " (31756, 1),\n",
       " (31943, 1),\n",
       " (31956, 1),\n",
       " (31973, 1),\n",
       " (32088, 1),\n",
       " (32108, 1),\n",
       " (32197, 1),\n",
       " (32232, 1),\n",
       " (32321, 1),\n",
       " (32553, 1),\n",
       " (32560, 1),\n",
       " (32670, 1),\n",
       " (32683, 1),\n",
       " (32692, 1),\n",
       " (32775, 1),\n",
       " (32938, 1),\n",
       " (33256, 1),\n",
       " (33351, 1),\n",
       " (33372, 1),\n",
       " (33416, 1),\n",
       " (33560, 1),\n",
       " (33617, 1),\n",
       " (33708, 1),\n",
       " (33776, 1),\n",
       " (33957, 1),\n",
       " (34103, 1),\n",
       " (34128, 1),\n",
       " (34158, 1),\n",
       " (34249, 1),\n",
       " (34251, 1),\n",
       " (34371, 1),\n",
       " (34420, 1),\n",
       " (34463, 1),\n",
       " (34674, 1),\n",
       " (34724, 1),\n",
       " (34795, 1),\n",
       " (34826, 1),\n",
       " (34856, 1),\n",
       " (34865, 1),\n",
       " (35005, 1),\n",
       " (35128, 1),\n",
       " (35149, 1),\n",
       " (35334, 1),\n",
       " (35420, 1),\n",
       " (35567, 1),\n",
       " (35680, 1),\n",
       " (35747, 1),\n",
       " (35773, 1),\n",
       " (35825, 1),\n",
       " (36073, 1),\n",
       " (36183, 1),\n",
       " (36389, 1),\n",
       " (36411, 1),\n",
       " (36425, 1),\n",
       " (36490, 1),\n",
       " (36524, 1),\n",
       " (36634, 1),\n",
       " (36662, 1),\n",
       " (36708, 1),\n",
       " (36714, 1),\n",
       " (36764, 1),\n",
       " (36968, 1),\n",
       " (37052, 1),\n",
       " (37061, 1),\n",
       " (37140, 1),\n",
       " (37260, 1),\n",
       " (37288, 1),\n",
       " (37296, 1),\n",
       " (37372, 1),\n",
       " (37376, 1),\n",
       " (37426, 1),\n",
       " (37497, 1),\n",
       " (37518, 1),\n",
       " (37553, 1),\n",
       " (37567, 1),\n",
       " (37617, 1),\n",
       " (37719, 1),\n",
       " (37761, 1),\n",
       " (37786, 1),\n",
       " (37915, 1),\n",
       " (37985, 1),\n",
       " (38189, 1),\n",
       " (38254, 1),\n",
       " (38284, 1),\n",
       " (38406, 1),\n",
       " (38484, 1),\n",
       " (38753, 1),\n",
       " (38780, 1),\n",
       " (38785, 1),\n",
       " (38801, 1),\n",
       " (38873, 1),\n",
       " (39031, 1),\n",
       " (39103, 1),\n",
       " (39112, 1),\n",
       " (39143, 1),\n",
       " (39204, 1),\n",
       " (39250, 1),\n",
       " (39361, 1),\n",
       " (39433, 1),\n",
       " (39569, 1),\n",
       " (39570, 1),\n",
       " (39611, 1),\n",
       " (39613, 1),\n",
       " (39620, 1),\n",
       " (39700, 1),\n",
       " (39728, 1),\n",
       " (39976, 1),\n",
       " (40001, 1),\n",
       " (40051, 1),\n",
       " (40057, 1),\n",
       " (40339, 1),\n",
       " (40340, 1),\n",
       " (40345, 1),\n",
       " (40363, 1),\n",
       " (40439, 1),\n",
       " (40483, 1),\n",
       " (40591, 1),\n",
       " (40625, 1),\n",
       " (40924, 1),\n",
       " (40943, 1),\n",
       " (41015, 1),\n",
       " (41030, 1),\n",
       " (41033, 1),\n",
       " (41327, 1),\n",
       " (41374, 1),\n",
       " (41496, 1),\n",
       " (41606, 1),\n",
       " (41656, 1),\n",
       " (41705, 1),\n",
       " (41747, 1),\n",
       " (41940, 1),\n",
       " (41972, 1),\n",
       " (41984, 1),\n",
       " (42070, 1),\n",
       " (42168, 1),\n",
       " (42184, 1),\n",
       " (42189, 1),\n",
       " (42364, 1),\n",
       " (42456, 1),\n",
       " (42472, 1),\n",
       " (42882, 1),\n",
       " (42892, 1),\n",
       " (42983, 1),\n",
       " (43020, 1),\n",
       " (43031, 1),\n",
       " (43085, 1),\n",
       " (43349, 1),\n",
       " (43373, 1),\n",
       " (43453, 1),\n",
       " (43460, 1),\n",
       " (43537, 1),\n",
       " (43603, 1),\n",
       " (43607, 1),\n",
       " (43809, 1),\n",
       " (43941, 1),\n",
       " (44125, 1),\n",
       " (44130, 1),\n",
       " (44138, 1),\n",
       " (44171, 1),\n",
       " (44232, 1),\n",
       " (44351, 1),\n",
       " (44377, 1),\n",
       " (44779, 1),\n",
       " (44815, 1),\n",
       " (44853, 1),\n",
       " (44881, 1),\n",
       " (44901, 1),\n",
       " (45541, 1),\n",
       " (45554, 1),\n",
       " (45596, 1),\n",
       " (45610, 1),\n",
       " (45685, 1),\n",
       " (45817, 1),\n",
       " (45931, 1),\n",
       " (46104, 1),\n",
       " (46153, 1),\n",
       " (46201, 1),\n",
       " (46243, 1),\n",
       " (46353, 1),\n",
       " (46356, 1),\n",
       " (46424, 1),\n",
       " (47095, 1),\n",
       " (47172, 1),\n",
       " (47188, 1),\n",
       " (47222, 1),\n",
       " (47328, 1),\n",
       " (47416, 1),\n",
       " (47547, 1),\n",
       " (47679, 1),\n",
       " (47732, 1),\n",
       " (47946, 1),\n",
       " (47983, 1),\n",
       " (48017, 1),\n",
       " (48030, 1),\n",
       " (48059, 1),\n",
       " (48111, 1),\n",
       " (48207, 1),\n",
       " (48226, 1),\n",
       " (48228, 1),\n",
       " (48229, 1),\n",
       " (48305, 1),\n",
       " (48604, 1),\n",
       " (48605, 1),\n",
       " (48612, 1),\n",
       " (48753, 1)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"UNBA.csv\", 'w') as f:\n",
    "times = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "F1s = []\n",
    "rmses = []\n",
    "for budget in [0.01]:\n",
    "    ts = 0\n",
    "    pres = 0\n",
    "    recs = 0\n",
    "    fs = 0\n",
    "    rs = 0\n",
    "    \n",
    "    # acquisition results : acquired_df_miss\n",
    "    st = time.time()\n",
    "    acquired_df_miss = one_pass_max_uncertainty(df_miss, df_arr, budget, query)\n",
    "    et = time.time()\n",
    "    \n",
    "    for query in tqdm(queries):\n",
    "        \n",
    "        imputed_data = IterativeImputer(random_state=0, max_iter=50).fit_transform(acquired_df_miss)\n",
    "        df_imputed = pd.DataFrame(imputed_data, columns = df.keys().to_list())\n",
    "        formalize_imputed_df(df_imputed, mapping, df.keys().to_list())\n",
    "\n",
    "        data_product = query_on_df(query, df_imputed)\n",
    "        ground_truth = query_on_df(query, df_arr)\n",
    "\n",
    "        ground_truth_indexes = set(ground_truth.index.values.tolist())\n",
    "        data_product_indexes = set(data_product.index.values.tolist())\n",
    "        intersect = ground_truth_indexes.intersection(data_product_indexes)\n",
    "        indexes_lst = list(intersect)\n",
    "        imputing_error = get_avg_error(df_arr.iloc[indexes_lst], df_imputed.iloc[indexes_lst], mapping)\n",
    "        precision = len(intersect)/len(ground_truth_indexes)\n",
    "        recall = len(intersect)/len(data_product_indexes)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        rmse = F1*(1-imputing_error)\n",
    "\n",
    "        ts += (et-st)\n",
    "        pres += precision\n",
    "        recs += recall\n",
    "        fs += F1\n",
    "        rs += rmse\n",
    "\n",
    "    times.append(ts/10)\n",
    "    precisions.append(pres/10)\n",
    "    recalls.append(recs/10)\n",
    "    F1s.append(fs/10)\n",
    "    rmses.append(rs/10)\n",
    "#     f.write(str(times[-1])+'\\t'+\n",
    "#             str(precisions[-1])+'\\t'+\n",
    "#             str(recalls[-1])+'\\t'+\n",
    "#             str(F1s[-1])+'\\t'+\n",
    "#             str(rmses[-1])+'\\t'+\n",
    "#             '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df_miss = df_miss_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acq_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'isna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx_miss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum(), budget_units\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'isna'"
     ]
    }
   ],
   "source": [
    "x_miss.isna().sum().sum(), budget_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1905"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miss.isna().sum().sum()-df_miss_copy.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188571, 190476)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_miss_copy = df_miss.copy()\n",
    "acquired_df_miss.isna().sum().sum(), df_miss.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. data imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 soft imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_error, grid_lambda = cv_softimpute(x_miss, grid_len=grid_len_softimputing)\n",
    "# lbda = grid_lambda[np.argmin(cv_error)]\n",
    "# imputed_data = softimpute((x_miss), lbda)[1]\n",
    "\n",
    "# imputed_data = SimpleImputer().fit_transform(x_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 ice imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imputed after formalization\n",
    "# float32 and float64 remains float\n",
    "# others to int64\n",
    "# imputed_data = IterativeImputer(random_state=0, max_iter=50).fit_transform(acquired_df_miss)\n",
    "imputed_data = IterativeImputer(random_state=0, max_iter=50).fit_transform(acquired_df_miss)\n",
    "df_imputed = pd.DataFrame(imputed_data, columns = df.keys().to_list())\n",
    "formalize_imputed_df(df_imputed, mapping, df.keys().to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5455.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age workclass education education_num marital_status occupation   \n",
       "0      39.0         7         9            12            NaN          1  \\\n",
       "1       NaN         6       NaN            12              2          4   \n",
       "2      38.0         4        11             8              0          6   \n",
       "3       NaN         4         1             6              2          6   \n",
       "4       NaN       NaN         9            12            NaN         10   \n",
       "...     ...       ...       ...           ...            ...        ...   \n",
       "48837  39.0         4         9           NaN              0         10   \n",
       "48838  64.0         0        11             8            NaN          0   \n",
       "48839  38.0         4         9           NaN            NaN         10   \n",
       "48840  44.0         4         9            12              0        NaN   \n",
       "48841  35.0         5       NaN           NaN              2          4   \n",
       "\n",
       "      relationship race  sex  capital_gain  capital_loss  hours_per_week   \n",
       "0                1    4    1        2174.0           0.0             NaN  \\\n",
       "1                0  NaN    1           0.0           0.0            13.0   \n",
       "2                1    4    1           0.0           NaN            40.0   \n",
       "3                0    2    1           0.0           0.0             NaN   \n",
       "4                5    2    0           0.0           0.0            40.0   \n",
       "...            ...  ...  ...           ...           ...             ...   \n",
       "48837            1  NaN    0           NaN           0.0            36.0   \n",
       "48838            2    2    1           0.0           NaN            40.0   \n",
       "48839            0    4  NaN           0.0           0.0             NaN   \n",
       "48840            3  NaN    1        5455.0           NaN            40.0   \n",
       "48841            0    4    1           NaN           NaN            60.0   \n",
       "\n",
       "      native_country  \n",
       "0                 39  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                 39  \n",
       "4                  5  \n",
       "...              ...  \n",
       "48837             39  \n",
       "48838             39  \n",
       "48839             39  \n",
       "48840             39  \n",
       "48841             39  \n",
       "\n",
       "[48842 rows x 13 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_product = query_on_df(query, df_imputed)\n",
    "ground_truth = query_on_df(query, df_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_indexes = set(ground_truth.index.values.tolist())\n",
    "data_product_indexes = set(data_product.index.values.tolist())\n",
    "intersect = ground_truth_indexes.intersection(data_product_indexes)\n",
    "indexes_lst = list(intersect)\n",
    "imputing_error = get_avg_error(df_arr.iloc[indexes_lst], df_imputed.iloc[indexes_lst], mapping)\n",
    "precision = len(intersect)/len(ground_truth_indexes)*(1-imputing_error)\n",
    "recall = len(intersect)/len(data_product_indexes)*(1-imputing_error)\n",
    "F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6135585123342105, 0.8142906182213288, 0.6998147488162082)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6131321733258313, 0.7941169714762272, 0.69198644233624)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6929133858267716"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1/(1-imputing_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9986622520079826"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-imputing_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open(\"UNBA.csv\", 'w') as f:\n",
    "# times = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# F1s = []\n",
    "# rmses = []\n",
    "# for budget in [0.01]:\n",
    "#     ts = 0\n",
    "#     pres = 0\n",
    "#     recs = 0\n",
    "#     fs = 0\n",
    "#     rs = 0\n",
    "    \n",
    "#     # acquisition results : acquired_df_miss\n",
    "#     st = time.time()\n",
    "#     acquired_df_miss = one_pass_max_uncertainty(df_miss, df_arr, budget, query)\n",
    "#     et = time.time()\n",
    "    \n",
    "#     for query in tqdm(queries):\n",
    "        \n",
    "#         imputed_data = IterativeImputer(random_state=0, max_iter=50).fit_transform(acquired_df_miss)\n",
    "#         df_imputed = pd.DataFrame(imputed_data, columns = df.keys().to_list())\n",
    "#         formalize_imputed_df(df_imputed, mapping, df.keys().to_list())\n",
    "\n",
    "#         data_product = query_on_df(query, df_imputed)\n",
    "#         ground_truth = query_on_df(query, df_arr)\n",
    "\n",
    "#         ground_truth_indexes = set(ground_truth.index.values.tolist())\n",
    "#         data_product_indexes = set(data_product.index.values.tolist())\n",
    "#         intersect = ground_truth_indexes.intersection(data_product_indexes)\n",
    "#         indexes_lst = list(intersect)\n",
    "#         imputing_error = get_avg_error(df_arr.iloc[indexes_lst], df_imputed.iloc[indexes_lst], mapping)\n",
    "#         precision = len(intersect)/len(ground_truth_indexes)\n",
    "#         recall = len(intersect)/len(data_product_indexes)\n",
    "#         F1 = 2 * (precision * recall) / (precision + recall)\n",
    "#         rmse = F1*(1-imputing_error)\n",
    "\n",
    "#         ts += (et-st)\n",
    "#         pres += precision\n",
    "#         recs += recall\n",
    "#         fs += F1\n",
    "#         rs += rmse\n",
    "\n",
    "#     times.append(ts/10)\n",
    "#     precisions.append(pres/10)\n",
    "#     recalls.append(recs/10)\n",
    "#     F1s.append(fs/10)\n",
    "#     rmses.append(rs/10)\n",
    "# #     f.write(str(times[-1])+'\\t'+\n",
    "# #             str(precisions[-1])+'\\t'+\n",
    "# #             str(recalls[-1])+'\\t'+\n",
    "# #             str(F1s[-1])+'\\t'+\n",
    "# #             str(rmses[-1])+'\\t'+\n",
    "# #             '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 1: Estimated Mean = 5.255813953488372, Plays = 86.0\n",
      "Arm 2: Estimated Mean = 4.862745098039215, Plays = 51.0\n",
      "Arm 3: Estimated Mean = 4.2727272727272725, Plays = 11.0\n",
      "Arm 4: Estimated Mean = 5.061224489795919, Plays = 147.0\n",
      "Arm 5: Estimated Mean = 5.107317073170732, Plays = 205.0\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "num_arms = 5\n",
    "bandit = UCBMultiArmBandit(num_arms, [1,2,3,4,5])\n",
    "\n",
    "# Simulate bandit plays\n",
    "num_plays = 500\n",
    "for _ in range(num_plays):\n",
    "    chosen_arm = bandit.select_arm()\n",
    "    # the reward should be if it has found a usable tuple\n",
    "    reward = np.random.randint(1,10)\n",
    "    bandit.update(chosen_arm, reward)\n",
    "\n",
    "# Print the estimated means for each arm\n",
    "for arm in range(num_arms):\n",
    "    estimated_mean = bandit.total_rewards[arm] / bandit.arm_counts[arm]\n",
    "    print(f\"Arm {arm + 1}: Estimated Mean = {estimated_mean}, Plays = {bandit.arm_counts[arm]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_acquisition(df_miss, df_arr, budget, queries):\n",
    "    df_miss_copy = df_miss.copy()\n",
    "    columns = []\n",
    "    for query in queries:\n",
    "        query_columns = [q['col'] for q in query]\n",
    "        columns += query_columns\n",
    "\n",
    "    total_acq_num = budget*df_miss.isna().sum().sum()      # get acquisition cell number\n",
    "    columns_count = dict(Counter(columns))\n",
    "    for k, v in columns_count.items():\n",
    "        _rows = np.where(np.isnan(df_miss_copy[k].to_numpy())) # get rows for missing column k\n",
    "        sample_num = int(v / len(columns)*total_acq_num)       # get acquisition cell number in column k\n",
    "        sample_rows = random.sample(range(len(_rows[0])), sample_num)\n",
    "        for r in sample_rows:\n",
    "            df_miss_copy[k].iat[_rows[0][r]] = df_arr[k].iat[_rows[0][r]]\n",
    "    \n",
    "    return df_miss_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code for UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "357.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
